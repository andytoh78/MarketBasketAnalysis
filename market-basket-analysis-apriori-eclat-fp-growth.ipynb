{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7195974,"sourceType":"datasetVersion","datasetId":4161681}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andrewtoh78/market-basket-analysis-apriori-eclat-fp-growth?scriptVersionId=156084756\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color : midnightblue; padding : 10pt; font-size: 28pt; font-family: robotosans; color: white; font-weight: bold\">Market Basket Analysis using<br>Apriori  .  Eclat  .  FP-Growth Algorithms</div>","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:02:38.412764Z","iopub.execute_input":"2023-12-15T15:02:38.413118Z","iopub.status.idle":"2023-12-15T15:02:38.454478Z","shell.execute_reply.started":"2023-12-15T15:02:38.413088Z","shell.execute_reply":"2023-12-15T15:02:38.451946Z"}}},{"cell_type":"markdown","source":"**Market Basket Analysis (MBA) is a data mining technique** used to **discover purchasing patterns** by analyzing extensive volume of transaction data. Primarily used in retail to understand customer purchase behavior, MBA helps in product placement, promotion strategies, and inventory management. The main objective is to **detect \"IF-THEN\" patterns**, find **correlations between different items purchased together**, using methods such as **Association Rule Learning**. By leveraging these techniques, retailers can extract meaningful insights from customer purchase data, leading to optimized decision-making in marketing, sales, and product management. For example : if a customer buys 'milk', then the customer is likely to buy 'bread'.</span>","metadata":{}},{"cell_type":"code","source":"!pip install graphviz","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:04.222661Z","iopub.execute_input":"2023-12-22T16:04:04.223568Z","iopub.status.idle":"2023-12-22T16:04:19.082113Z","shell.execute_reply.started":"2023-12-22T16:04:04.223516Z","shell.execute_reply":"2023-12-22T16:04:19.080708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import display\nfrom graphviz import Digraph\n\ndot = Digraph(comment='Market Basket Analysis Process')\ndot.attr(rankdir='LR')\nnode_attr = {'shape': 'box', 'height': '1', 'fontsize': '14', 'fontname': 'Arial'}\ndot.attr('node', **node_attr)\ndot.node('A', '  Collect  \\nData')\ndot.node('B', 'Clean &\\nPreprocess\\nData')\ndot.node('C', 'Use Association\\nRule Mining\\nAlgorithms')\ndot.node('D', 'Calculate\\nSupport\\n& Confidence')\ndot.node('E', 'Generate\\nAssociation\\nRules')\ndot.node('F', 'Interpret\\nResults')\ndot.node('G', 'Inform\\nBusiness\\nDecisions')\n\ndot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG'])\ndisplay.display_svg(dot)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.089102Z","iopub.execute_input":"2023-12-22T16:04:19.089444Z","iopub.status.idle":"2023-12-22T16:04:19.150606Z","shell.execute_reply.started":"2023-12-22T16:04:19.089402Z","shell.execute_reply":"2023-12-22T16:04:19.149379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"| **Process** |**Description** |\n|:--- |:--- |\n| **Collect Data** | The foundation of Market Basket Analysis is the collection of customer transaction data. This data typically includes key details such as the items purchased in each transaction, the time and date of purchase, and other relevant customer and transaction information. The richness and accuracy of this data are crucial, as they form the basis for all subsequent analysis and insights.|\n| **Preprocess Data** | Data cleaning and preprocessing are critical steps to ensure the quality and usability of the transaction data. This phase involves removing irrelevant information that does not contribute to the analysis, such as extraneous details or erroneous entries, handling missing values, and converting/ restructuring the data into a consistent format that is compatible with the Algorithm.|\n| **Use Association Rule Mining Algorithms** | Applying data mining algorithms, like Apriori or FP-Growth, to identify patterns within the data. These algorithms are designed to uncover frequent itemsets, which are groups of items that commonly appear together in transactions. These itemsets form the basis for deriving meaningful insights about customer purchasing patterns.|\n| **Calculate Support and Confidence** | Two key metrics are calculated in this step: support and confidence.<br>- Support refers to the frequency of occurrence of an itemset within the dataset, indicating how common or popular a combination of items is<br>- Confidence measures the conditional probability that a specific item is purchased when another item or set of items is bought|\n| **Generate Association Rules** | Based on the frequent itemsets and their corresponding support and confidence values, association rules are generated. These rules are logical statements that express the likelihood of items being purchased together. They are fundamental in understanding and leveraging the relationships unearthed in the data.|\n| **Interpret Results** | Analyzing the patterns and associations identified by the Market Basket Analysis. This step is crucial for gaining insights into customer behavior and preferences. By understanding these patterns, businesses can gain a deeper understanding of how different products are interconnected in the eyes of the customers.|\n| **Inform Business Decisions** | The final step is applying the insights derived from the Market Basket Analysis to inform and enhance business decisions. This can include using the findings for product recommendations, optimizing the store layout based on commonly purchased items, and developing targeted marketing campaigns. These actions can lead to increased sales, improved customer satisfaction, and more efficient store management.|\n","metadata":{}},{"cell_type":"markdown","source":"## **Association Rule Learning**\n---\n\nAssociation rule learning is a technique in data mining for discovering interesting relationships between variables in large datasets. The primary goal is to find frequent patterns, correlations, or associations from data sets found in various kinds of databases such as relational databases, transactional databases, and other forms of repositories. For instance, have you ever considered the [possibility of a connection between the purchase of diapers and beer](https://tdwi.org/articles/2016/11/15/beer-and-diapers-impossible-correlation.aspx)?\n\nAn association rule has two parts: **antecedent (IF)** and **consequent (THEN)**. The rule suggests that if the antecedent happens, the consequent is likely to occur as well. These rules are typically used to analyze retail basket or transaction data, but are also applicable in other areas like web usage mining, intrusion detection, and bioinformatics.\n\nAssociation rules can be classified into 3 broad categories and the strength can be measured in terms of its **support** and **confidence**.\n- **Trival** (everyone knows about it!) \n- **Inexplicable** (does not make any logical sense but seems to correlated)\n- **Actionable** (insights that suggest a course of action) \n\n&nbsp;\n\n## **Support**\n---\nSupport (Coverage) refers to the **frequency** with which **an itemset appears in the dataset**. Ranged **between 0 and 1**, it provides an indication of how common or popular an itemset is within the given dataset, with **0 being the least common** (indicating that the itemset does not appear in any transactions) and **1 being the most common** (indicating that the itemset appears in all transactions). An itemset in the context of association rule learning is a set of one or more items that occur together in a transactional dataset.\n\nAn itemset is considered **frequent** only if the support is equal or greater than an agreed upon (user-defined) minimal value, also known as **minimum support threshold**. This principle assumes that not all item combinations are equally significant or interesting; some occur together more often than others. By focusing only the frequent itemsets, we aim to uncover the most relevant and potentially insightful patterns within the dataset.<br><br>\n\n$$\\begin{equation}\n\\textsf{Support}(X)=\\dfrac{\\textsf{Total number of transactions containing } X}{\\textsf{Total number of transactions}}\n\\end{equation}$$\n\n\n<br>For an association rule $X$ (antecedent) ⇒ $Y$ (consequent), the support is the proportion of transactions in the dataset that contain both $X$ and $Y$.<br><br>\n\n$$\\begin{equation}\n\\textsf{Support}(X \\cup Y)=\\dfrac{\\textsf{Total number of transactions containing } X\\textsf{ and } Y}{\\textsf{Total number of transactions}}=\\dfrac{(X \\cup Y)}{N}\n\\end{equation}$$","metadata":{}},{"cell_type":"markdown","source":"Consider the following transaction dataset (df), where each row represents a basket or transaction with a unique Transaction ID (TID), and each column represents a unique item. The values '1' and '0' indicate whether a particular item was purchased ('1') or not ('0') in each transaction.<br><br>\n\n| TID         | Milk | Apple | Butter | Bread | Orange | Spinach |\n|:----------- |:---- |:----- |:------ |:------|:------ |:------- |\n| 1           | 1    | 1     | 1      | 1     | 1      | 1       |\n| 2           | 0    | 1     | 0      | 1     | 1      | 1       |\n| 3           | 0    | 0     | 0      | 1     | 0      | 0       |\n| 4           | 1    | 1     | 0      | 0     | 1      | 1       |\n| 5           | 0    | 1     | 1      | 1     | 1      | 1       |\n| 6           | 1    | 0     | 1      | 1     | 1      | 0       |\n| 7           | 1    | 0     | 1      | 0     | 0      | 0       |\n| 8           | 1    | 1     | 0      | 1     | 1      | 0       |\n| 9           | 0    | 1     | 0      | 0     | 1      | 0       |\n| 10          | 1    | 1     | 1      | 1     | 1      | 0       |\n\nTo calculate the **Support** (or frequency) for each item, we will need to count the number of transactions in which each item appears and then divide that count by the total number of transactions. Below is the Python code snippet to calculate the Support for each item in the transaction dataset.","metadata":{}},{"cell_type":"code","source":"# Import pandas\nimport pandas as pd\n\n# Create the transaction dataframe\ndata = {\n    \"TID\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    \"Milk\": [1, 0, 0, 1, 0, 1, 1, 1, 0, 1],\n    \"Apple\": [1, 1, 0, 1, 1, 0, 0, 1, 1, 1],\n    \"Butter\": [1, 0, 0, 0, 1, 1, 1, 0, 0, 1],\n    \"Bread\": [1, 1, 1, 0, 1, 1, 0, 1, 0, 1],\n    \"Orange\": [1, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n    \"Spinach\": [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n}\n\ndf = pd.DataFrame(data)\n\n# Compute the total count of transactions in the dataset\ntxn_count = len(df)\n\n# Get the list of item names, excluding TID\nitems = df.columns.drop('TID')\n\n# Initialize an empty list to store support values\nsupport_values = []\n\n# Calculate support for each item\nfor itemset in items:\n    support_value = df[itemset].sum() / txn_count\n    support_values.append((itemset, support_value))\n\n# Convert the results into a dataframe\ndf_support = pd.DataFrame(support_values, columns=['Itemset', 'Support'])\n\n# Display the results\ndf_support","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.15234Z","iopub.execute_input":"2023-12-22T16:04:19.153507Z","iopub.status.idle":"2023-12-22T16:04:19.624996Z","shell.execute_reply.started":"2023-12-22T16:04:19.153456Z","shell.execute_reply":"2023-12-22T16:04:19.623648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the frequency of itemsets containing two items\n# Import combinations function from itertools to generate all possible combinations\nfrom itertools import combinations\n\n# Compute the support for each itemset of size 2\ncombo2_support_values_list = []\nfor combo in combinations(items, 2):\n    combo_support = df[list(combo)].all(axis=1).mean()\n    combo2_support_values_list.append((list(combo), combo_support))\n\n# Convert the results into a dataframe\ndf_combo2_support = pd.DataFrame(combo2_support_values_list, columns=['Itemset', 'Support'])\n\n# Display the results\ndf_combo2_support","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.628461Z","iopub.execute_input":"2023-12-22T16:04:19.629088Z","iopub.status.idle":"2023-12-22T16:04:19.672796Z","shell.execute_reply.started":"2023-12-22T16:04:19.629033Z","shell.execute_reply":"2023-12-22T16:04:19.671707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the frequency of itemsets containing three items\n# Import combinations function from itertools to generate all possible combinations\nfrom itertools import combinations\n\n# Compute the support for each itemset of size 3\ncombo3_support_values_list = []\nfor combo in combinations(items, 3):\n    combo_support = df[list(combo)].all(axis=1).mean()\n    combo3_support_values_list.append((list(combo), combo_support))\n\n# Convert the results into a dataframe\ndf_combo3_support = pd.DataFrame(combo3_support_values_list, columns=['Itemset', 'Support'])\n\n# Display the results\ndf_combo3_support","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.674101Z","iopub.execute_input":"2023-12-22T16:04:19.674439Z","iopub.status.idle":"2023-12-22T16:04:19.731455Z","shell.execute_reply.started":"2023-12-22T16:04:19.674409Z","shell.execute_reply":"2023-12-22T16:04:19.729961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining the support for both single-item, two-item and three-item itemsets, and assuming itemsets are only considered frequent if they exist in 50% of the transactions i.e. **minimum suport threshold = 0.5**, we are left with ten itemsets that meet or exceed this threshold. These frequent itemsets will be our main focus in identifying the key patterns and trends in consumer purchasing behaviour, providing valuable insights for strategic decision-making in areas such as product placement, marketing and promotions, and inventory optimization.","metadata":{}},{"cell_type":"code","source":"# Combining both single-item and two-item itemsets\ndf_support_combined = pd.concat([df_support, df_combo2_support, df_combo3_support])\n\n# Define minimum support threshold\nmin_support_threshold = 0.5\n\n# Drop itemsets with support less than or equal to 0.5\ndf_support_final = df_support_combined[df_support_combined['Support'] >= min_support_threshold]\n\n# Display the results\ndf_support_final","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.733145Z","iopub.execute_input":"2023-12-22T16:04:19.733596Z","iopub.status.idle":"2023-12-22T16:04:19.753672Z","shell.execute_reply.started":"2023-12-22T16:04:19.73355Z","shell.execute_reply":"2023-12-22T16:04:19.752484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Establish Association Rules**\n---\nWith the frequent itemsets, we can now formulate potential association rules $(X \\Rightarrow Y)$. These rules are structured as if-then statements and are instrumental in evaluating the probability of relationships between different items within the dataset.\n\nSince an association rule requires and is defined by an antecedent and a consequent, it can only be generated for itemsets that contain **at least two items**.","metadata":{}},{"cell_type":"code","source":"lift_values = []\n# Calculate lift for two-item itemsets\nfor index, row in df_support_final.iterrows():\n    if isinstance(row['Itemset'], list) and len(row['Itemset']) == 2:\n        item1, item2 = row['Itemset']\n        support_item1 = df_support[df_support['Itemset'] == item1]['Support'].values[0]\n        support_item2 = df_support[df_support['Itemset'] == item2]['Support'].values[0]\n        lift = (row['Support']) / ((support_item1)*(support_item2))\n        lift_values.append({'Rule': f'[{item1}, {item2}]', 'Lift': lift})\n\n# Calculate lift for three-item itemsets\nfor index, row in df_support_final.iterrows():\n    if isinstance(row['Itemset'], list) and len(row['Itemset']) == 3:\n        item1, item2, item3 = row['Itemset']\n        support_item1 = df_support[df_support['Itemset'] == item1]['Support'].values[0]\n        support_item2 = df_support[df_support['Itemset'] == item2]['Support'].values[0]        \n        support_item3 = df_support[df_support['Itemset'] == item3]['Support'].values[0]        \n        lift = (row['Support']) / ((support_item1)*(support_item2)*(support_item3))\n        lift_values.append({'Rule': f'[{item1}, {item2}, {item3}]', 'Lift': lift})\n\n# Convert the results into a DataFrame\ndf_lift = pd.DataFrame(lift_values, columns=['Rule', 'Lift'])\n\n# Display the results\ndf_lift","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.755331Z","iopub.execute_input":"2023-12-22T16:04:19.755676Z","iopub.status.idle":"2023-12-22T16:04:19.784953Z","shell.execute_reply.started":"2023-12-22T16:04:19.755647Z","shell.execute_reply":"2023-12-22T16:04:19.783403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a slight to moderate positive association between the frequent two and three-item itemsets, indicating that the co-occurrence of these items in transactions is more frequent (lift > 1) than what would be expected by chance.\n\nAssociation Rule Learning is a powerful technique for uncovering valuable insights from transactional data, enabling the discovery of interesting patterns and relationships among items. To tackle complex datasets and real-world scenarios, there are advanced algorithms such as **Apriori**, **Eclat**, and **FP-growth** that we can leverage on to assist in finding frequent itemsets, and uncover hidden associations, and optimize decision-making processes.","metadata":{}},{"cell_type":"markdown","source":"## **Apriori Algorithm**\n---\nThe Apriori algorithm, introduced by R. Agrawal and R. Srikant in 1994, is a classic algorithm in data mining used for mining frequent itemsets and discovering association rules in a database. This page aims to provide an overview of the Apriori algorithm, its method, key parameters, advantages, limitations, and a basic implementation guide using Python.\n\nThe Apriori algorithm operates by identifying frequent itemsets in a dataset and then extending them to larger itemsets, provided they appear sufficiently frequently in the database. It explores the itemset space in a breadth-first manner. Initially, it identifies frequent itemsets of size 1, then uses these to generate candidate itemsets of size 2, and so on, progressively increasing the size of itemsets and checking their support.\n\nA key principle of Apriori is that **a subset of a frequent itemset must also be frequent**. For instance, if the itemset {A, B, C} is frequent, it implies that all of its subsets, such as {A, B}, {A, C}, {B, C}, {A}, {B}, and {C}, must also be frequent. This is because any transaction containing {A, B, C} also contains all its subsets. This principle efficiently reduces the number of support calculations needed and speeds up the discovery of association rules in large datasets.\n\n### **Key Parameters**\n| **Parameter**             | **Description**                                                               |\n|:--------------------------|:------------------------------------------------------------------------------|\n| `min_support`             | This is the user-defined threshold (a decimal between 0 and 1) that determines the minimum frequency at which an itemset must be present in the dataset to be considered 'frequent'.|\n| `confidence_threshold`    | This is the user-defined minimum level of confidence that an association rule must exceed to be considered significant.\n| `lift_threshold`          | This is the user-defined minimum lift value (typically >1) focusing on rules that have a positive relationship between the antecedent and consequent.|\n","metadata":{}},{"cell_type":"code","source":"# Install machine learning extensions (mlxtend) library\n# !pip install mlxtend","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.786925Z","iopub.execute_input":"2023-12-22T16:04:19.787424Z","iopub.status.idle":"2023-12-22T16:04:19.800769Z","shell.execute_reply.started":"2023-12-22T16:04:19.787342Z","shell.execute_reply":"2023-12-22T16:04:19.799774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset and view first 5 rows\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", category=Warning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", None)\n\ndf = pd.read_csv(\"/kaggle/input/market-basket-optimisation/Market_Basket_Optimisation.csv\", header=None, index_col=None, names=[f\"Item_{i}\" for i in range(1, 21)])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.80252Z","iopub.execute_input":"2023-12-22T16:04:19.802898Z","iopub.status.idle":"2023-12-22T16:04:19.864767Z","shell.execute_reply.started":"2023-12-22T16:04:19.802862Z","shell.execute_reply":"2023-12-22T16:04:19.863552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View shape of the dataset\nprint(f\"The dataset contains {df.shape[0]} transactions and each transaction has a maximum of {df.shape[1]} items or less.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.866014Z","iopub.execute_input":"2023-12-22T16:04:19.866356Z","iopub.status.idle":"2023-12-22T16:04:19.872291Z","shell.execute_reply.started":"2023-12-22T16:04:19.866326Z","shell.execute_reply":"2023-12-22T16:04:19.871035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate transaction lists\ntxns = df.fillna(\"\").values.tolist()\ntxns = [[item for item in txn if item != ''] for txn in txns]\ntxns = [[item.strip() for item in txn] for txn in txns]\n\n# Create a list of unique ids for the transactions\nids = [i + 1 for i in range(len(txns))]\n\n# Initialize an empty list\ndata =[]\n# Iterate through transactions and add them to the DataFrame with IDs\nfor i, txn in enumerate(txns):\n    data.extend([{'TID': ids[i], 'Item': item} for item in txn])\n\ndf_txn = pd.DataFrame(data)\ndf_txn.head(25)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.873975Z","iopub.execute_input":"2023-12-22T16:04:19.874422Z","iopub.status.idle":"2023-12-22T16:04:19.995467Z","shell.execute_reply.started":"2023-12-22T16:04:19.874372Z","shell.execute_reply":"2023-12-22T16:04:19.994173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform one hot encoding using TransactionEncoder\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# Create a TransactionEncoder\nte = TransactionEncoder()\n\n# Fit and transform the transaction data\nte_array = te.fit(txns).transform(txns)\n\n# Extract the column names\nte_columns = te.columns_\n\n# Create a DataFrame from the one-hot encoded array\ndf1 = pd.DataFrame(te_array, columns=te.columns_)\n\n# Display the results\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:19.997198Z","iopub.execute_input":"2023-12-22T16:04:19.997647Z","iopub.status.idle":"2023-12-22T16:04:20.69244Z","shell.execute_reply.started":"2023-12-22T16:04:19.997611Z","shell.execute_reply":"2023-12-22T16:04:20.691221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output of the above code snippet provides a binary representation of the transaction data. Each row represents a transaction, and each column corresponds to an item. If an item is present in a transaction, the corresponding cell value will be indicated as True; otherwise, it will be False. This **one-hot encoded** format is commonly used for various data mining tasks, including association rule mining. This is also the required format when using the Apriori alogrithm in identifying frequent itemsets and association rules.","metadata":{}},{"cell_type":"code","source":"# Find the top most frequent items\ntop_items = df_txn['Item'].value_counts().reset_index()\n\n# Convert the top items into DataFrame and sort by item count in descending order\ndf_top_items = pd.DataFrame(top_items)\ndf_top_items.columns = ['Item', 'Count']\n\n# Calculate the percentage of transactions for each item\ntotal_transactions = len(df)\ndf_top_items['% Count'] = (df_top_items['Count']*100 / total_transactions).round(2)\n\n# Display the results\ndf_top_items.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:20.69969Z","iopub.execute_input":"2023-12-22T16:04:20.700117Z","iopub.status.idle":"2023-12-22T16:04:20.835585Z","shell.execute_reply.started":"2023-12-22T16:04:20.700081Z","shell.execute_reply":"2023-12-22T16:04:20.834456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mineral water is the most frequently purchased item, and it appears in 1788 (~24%) transactions. Several food items like eggs, spaghetti, french fries, chocolate, and green tea also have high purchase counts. We can also visualize the frequent items using bar charts, heatmaps, pie charts, tree maps, word cloud to better understand their distribution within the dataset.","metadata":{}},{"cell_type":"code","source":"# Create the countplot to display top 30 most frequent items\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(18, 6))\nplt.bar(data=df_top_items.head(30), x=\"Item\", height=\"Count\", edgecolor=\"black\", color=\"skyblue\")\nax.bar_label(ax.containers[0], fontsize=10, fontweight=600)\nplt.title(\"Top 30 Most Frequent Items\\n\", fontsize=14, fontweight=700)\nplt.xlabel(\"\\nItem\", fontsize=12, fontweight=700)\nplt.ylabel(\"Quantity\\n\", fontsize=12, fontweight=700)\nplt.xticks(rotation=90)\nplt.gca().grid(False)\nplt.gca().spines[[\"left\", \"bottom\"]].set_color(\"black\")\nplt.gca().spines[[\"right\", \"top\"]].set_visible(False)\nplt.gca().set_facecolor(\"white\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:20.837256Z","iopub.execute_input":"2023-12-22T16:04:20.837903Z","iopub.status.idle":"2023-12-22T16:04:21.71313Z","shell.execute_reply.started":"2023-12-22T16:04:20.837857Z","shell.execute_reply":"2023-12-22T16:04:21.711495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the word cloud to visualize frequent items\nfrom wordcloud import WordCloud\n\n# Combine Item values into a string with space separator\nall_values = [item for txn in txns for item in txn]\nall_values_list = ' '.join(all_values)\n\n# Create a word cloud object\nwordcloud = WordCloud(width=300, height=300, background_color=\"white\", min_font_size=8, colormap='viridis').generate(all_values_list)\n\n# Display the word cloud\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:21.715353Z","iopub.execute_input":"2023-12-22T16:04:21.716344Z","iopub.status.idle":"2023-12-22T16:04:22.49577Z","shell.execute_reply.started":"2023-12-22T16:04:21.716291Z","shell.execute_reply":"2023-12-22T16:04:22.494397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the tree map to display top 30 most frequent items\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport squarify\n\n# Convert 'Count' column to integers\ndf_top_items['Count'] = df_top_items['Count'].astype(int)\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Top 30 items\ndf_top_30items = df_top_items.head(30)\n\n# Calculate treemap sizes\nsizes = df_top_30items['Count']\n\n# Calculate treemap labels\nlabels = [f\"{item}\\n({count})\" for item, count in zip(df_top_30items['Item'], df_top_30items['Count'])]\n\n# Plot the treemap\nsquarify.plot(sizes=sizes, label=labels, alpha=0.8, color=sns.color_palette(\"Spectral\", len(df_top_30items)), text_kwargs={'fontsize':10}, ax=ax)\n\nplt.axis('off')\nplt.title(\"Tree Map for Top 30 Most Frequent Items\\n\", fontsize=14, fontweight=700)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:22.497499Z","iopub.execute_input":"2023-12-22T16:04:22.497862Z","iopub.status.idle":"2023-12-22T16:04:23.181519Z","shell.execute_reply.started":"2023-12-22T16:04:22.497829Z","shell.execute_reply":"2023-12-22T16:04:23.18012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate frequent itemsets\nfrom mlxtend.frequent_patterns import apriori\n\n# Applying Apriori algorithm assuming an item has to appear in at least 4% of the total transaction to be considered as frequent\nmin_support_threshold = 0.04\nfrequent_itemsets = apriori(df1, min_support=min_support_threshold, use_colnames=True)\nfrequent_itemsets","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:23.18313Z","iopub.execute_input":"2023-12-22T16:04:23.183485Z","iopub.status.idle":"2023-12-22T16:04:23.303759Z","shell.execute_reply.started":"2023-12-22T16:04:23.183455Z","shell.execute_reply":"2023-12-22T16:04:23.302475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most frequent individual items are mineral water (23.84%), eggs (17.97%), and spaghetti (17.41%). There are also five frequent itemsets with two items e.g. (chocolate, mineral water) with a support of 5.27%, indicating that these items often appear together in transactions.","metadata":{}},{"cell_type":"code","source":"# Create the countplot for top 30 most frequent itemsets\nimport matplotlib.pyplot as plt\n\n# Convert frozensets to strings and remove 'frozenset' from the representation\nfrequent_itemsets['itemset'] = frequent_itemsets['itemsets'].apply(lambda x: ', '.join(list(x)))\n\n# Sort and filter top 15 most frequent itemsets\nfreq_sorted = frequent_itemsets.sort_values(by=\"support\", ascending=False)\nfreq_sorted_top30 = freq_sorted.head(30)\nfreq_sorted_top30[\"support %\"] = (freq_sorted_top30[\"support\"]*100).round(2)\n\nfig, ax = plt.subplots(figsize=(18, 6))\nplt.bar(data=freq_sorted_top30, x=\"itemset\", height=\"support %\", edgecolor=\"black\", color=\"skyblue\")\nax.bar_label(ax.containers[0], fontsize=10, fontweight=600)\nplt.title(\"Top 30 Most Frequent Items\\n\", fontsize=14, fontweight=700)\nplt.xlabel(\"\\nItem\", fontsize=12, fontweight=700)\nplt.ylabel(\"Quantity\\n\", fontsize=12, fontweight=700)\nplt.xticks(rotation=90)\nplt.gca().grid(False)\nplt.gca().spines[[\"left\", \"bottom\"]].set_color(\"black\")\nplt.gca().spines[[\"right\", \"top\"]].set_visible(False)\nplt.gca().set_facecolor(\"white\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:23.304995Z","iopub.execute_input":"2023-12-22T16:04:23.305478Z","iopub.status.idle":"2023-12-22T16:04:24.401274Z","shell.execute_reply.started":"2023-12-22T16:04:23.305432Z","shell.execute_reply":"2023-12-22T16:04:24.400122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate association rules from the frequent itemsets assuming the likelihood of purchasing the antecedent, followed by the consequent has to be at least 30% to be considered significant or interesting \nfrom mlxtend.frequent_patterns import association_rules\nconfidence_threshold = 0.3 \nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n\n# Sorting rules by confidence, support, and lift\nsorted_rules = rules.sort_values(['confidence', 'support', 'lift'], ascending=[False, False, False])\nsorted_rules","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:24.402827Z","iopub.execute_input":"2023-12-22T16:04:24.403926Z","iopub.status.idle":"2023-12-22T16:04:24.433713Z","shell.execute_reply.started":"2023-12-22T16:04:24.403881Z","shell.execute_reply":"2023-12-22T16:04:24.432391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customers who purchase ground beef are 41.66% likely to also purchase mineral water and this association is supported by a lift value of 1.75, which signifies that these items are frequently bought together. Similiar observations were found for some other items - milk, spaghetti, and chocolate, in association with mineral water. To visualize association rules, we can leverage the scatter plots, network graphs, heatmaps, and bar charts to understand their distribution within the dataset and their strength of association.","metadata":{}},{"cell_type":"code","source":"# Create a scatterplot to visualize the relationship between support and confidence in association rules\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.scatterplot(data=sorted_rules, x=sorted_rules[\"support\"], y=sorted_rules[\"confidence\"], size=\"confidence\", sizes=(0, 1000), legend=False)\n\n# Annotate the points with labels\nfor i, row in sorted_rules.iterrows():\n    # Convert frozenset to list, then to string and remove the frozenset and other unwanted characters\n    antecedents = ', '.join(list(row['antecedents']))\n    consequents = ', '.join(list(row['consequents']))\n    label = f\"{antecedents}\\n→ {consequents}\" # \\nSupport: {row['support']:.3f}, Confidence: {row['confidence']:.3f}\"\n    plt.annotate(label, (row['support'], row['confidence']), textcoords=\"offset points\", xytext=(15, 0), ha='left', fontsize=10)\nplt.title(\"Association Rules Support vs Confidence\\n\", fontsize=14, fontweight=700)\nplt.xlabel(\"\\nSupport\", fontsize=12, fontweight=700)\nplt.ylabel(\"Confidence\\n\", fontsize=12, fontweight=700)\nplt.gca().grid(False)\nplt.gca().spines[[\"left\", \"bottom\"]].set_color(\"black\")\nplt.gca().spines[[\"right\", \"top\"]].set_visible(False)\nplt.gca().set_facecolor(\"white\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:24.435464Z","iopub.execute_input":"2023-12-22T16:04:24.435876Z","iopub.status.idle":"2023-12-22T16:04:24.96397Z","shell.execute_reply.started":"2023-12-22T16:04:24.435841Z","shell.execute_reply":"2023-12-22T16:04:24.962757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a network graph to visualize the characteristics of the association rules\nimport networkx as nx\nimport matplotlib.cm as cm\nfrom matplotlib.cm import get_cmap\nimport matplotlib.colors as mcolors\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=Warning)\n\n# Create a graph\nG = nx.Graph()\n\nfor _, row in rules.iterrows():\n    # Convert frozenset to string\n    antecedents_str = ', '.join(list(row['antecedents']))\n    consequents_str = ', '.join(list(row['consequents']))\n    \n    # Add nodes and edges with the string labels\n    G.add_node(antecedents_str)\n    G.add_node(consequents_str)\n    G.add_edge(antecedents_str, consequents_str, weight=row['lift'])\n\ncolormap = get_cmap('viridis')\nlift_values = [data[\"weight\"] for _, _, data in G.edges(data=True)]\nlift_min = min(lift_values)\nlift_max = max(lift_values)\nlift_norm = mcolors.Normalize(vmin=lift_min, vmax=lift_max)\n\n# Define edge colors and widths based on lift values\nedge_colors = lift_values\nedge_widths = 3\n\n# Plot the graph\nfig, ax = plt.subplots(figsize=(14, 8))\npos = nx.spring_layout(G, k=0.15, iterations=20)\nnx.draw_networkx_nodes(G, pos, node_color=\"azure\", node_size=50)\nnx.draw_networkx_labels(G, pos, font_size=8)\nnx.draw(G, pos, edge_color=edge_colors, width=edge_widths, edge_cmap=colormap, edge_vmin=lift_min, edge_vmax=lift_max)\ncbar = plt.colorbar(plt.cm.ScalarMappable(cmap=colormap, norm=lift_norm), ax=ax, shrink=0.4)\ncbar.set_label(\"Lift Value\")\n\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:24.965371Z","iopub.execute_input":"2023-12-22T16:04:24.965731Z","iopub.status.idle":"2023-12-22T16:04:25.482472Z","shell.execute_reply.started":"2023-12-22T16:04:24.9657Z","shell.execute_reply":"2023-12-22T16:04:25.48127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a heatmap to visualize the relationships between the antecedents and consequents\n# Convert frozensets to strings and remove 'frozenset' from the representation\nrules['antecedents'] = rules['antecedents'].apply(lambda x: ', '.join(list(x)))\nrules['consequents'] = rules['consequents'].apply(lambda x: ', '.join(list(x)))\n\n# Show frequency or strength of item associations\npivot_table = rules.pivot(index=\"consequents\", columns=\"antecedents\", values=\"confidence\")\nplt.figure(figsize=(14, 2))\nsns.heatmap(pivot_table, annot=True, cmap=\"viridis\")\nplt.xlabel(\"\\nantecedents\", fontsize=12, fontweight=600)\nplt.ylabel(\"consequents\\n\\n\", fontsize=12, fontweight=600)\nplt.gca().set_facecolor(\"white\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:25.484372Z","iopub.execute_input":"2023-12-22T16:04:25.484849Z","iopub.status.idle":"2023-12-22T16:04:25.864866Z","shell.execute_reply.started":"2023-12-22T16:04:25.4848Z","shell.execute_reply":"2023-12-22T16:04:25.863455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Summary**\n- Mineral water is a frequent consequent, suggesting it is commonly bought with many different items.<br><p>\n- The strongest association rule based on confidence is (ground beef) -> (water) with a confidence of about 0.42, meaning that when ground beef is bought, water is also likely to be bought in 42% of the cases.<br><p>\n- Milk, chocolate, and spaghetti also have strong associations with mineral water, with confidence levels above 32%.","metadata":{}},{"cell_type":"markdown","source":"## **Eclat Algorithm**\n---\nThe Eclat (Equivalence Class Transformation) algorithm is another classic data mining algorithm used for mining frequent itemsets and discovering association rules in a database. It differs from the Apriori algorithm in terms of its methodology and efficiency. This page aims to provide an overview of the Eclat algorithm, its method, key parameters, advantages, limitations, and a basic implementation guide using Python.\n\nThe Eclat algorithm employs a depth-first search strategy to find frequent itemsets in a dataset. Instead of generating candidate itemsets as in Apriori, Eclat uses a **vertical data format to represent transactions**. It maintains an index structure, often called the **tidset**, which records the transactions in which each item appears. Eclat then recursively combines frequent itemsets by **intersecting their tidsets**. This approach scans the database only once, eliminates the need for candidate generation, making it efficient for mining frequent itemsets in large databases.\n\n### **Key Parameters**\n| **Parameter**             | **Description**                                                               |\n|:--------------------------|:------------------------------------------------------------------------------|\n| `min_support`             | User-defined threshold (a decimal between 0 and 1) that determines the minimum frequency at which an itemset must be present in the dataset to be considered 'frequent'.|\n| `min_combination`    | User-defined minimum size of the itemsets to be considered frequent.<br><br>Setting a higher value for `min_combination` will result in the algorithm only considering larger itemsets as frequent. This can lead to discovering fewer but potentially more significant association rules or patterns. It filters out smaller itemsets, which may include common but less interesting associations.<br><br> Setting a lower value for `min_combination` allows the algorithm to find smaller frequent itemsets. This can lead to a larger number of discovered itemsets, including more specific and potentially noise patterns. It may be useful for finding fine-grained associations but can also result in a higher volume of results to analyze.|\n| `max_combination`          | User-defined maximum size of the itemsets to be considered.<br><br>Setting a higher value for `max_combination` allows the algorithm to consider larger itemsets as frequent. This can be useful when you have prior knowledge that certain associations or patterns involve a larger number of items. However, it may also increase computational complexity and runtime.<br><br>Setting a lower value for `max_combination` limits the size of itemsets considered by the algorithm. It can lead to faster execution and a smaller number of results, focusing on more concise patterns. However, you might miss associations that involve larger sets of items.|","metadata":{}},{"cell_type":"code","source":"# Install the pyECLAT library\n# !pip install pyECLAT","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:25.866458Z","iopub.execute_input":"2023-12-22T16:04:25.867114Z","iopub.status.idle":"2023-12-22T16:04:25.873409Z","shell.execute_reply.started":"2023-12-22T16:04:25.86704Z","shell.execute_reply":"2023-12-22T16:04:25.871994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset and view first 5 rows\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", category=Warning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", None)\n\ndf = pd.read_csv(\"/kaggle/input/market-basket-optimisation/Market_Basket_Optimisation.csv\", header=None)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:25.875516Z","iopub.execute_input":"2023-12-22T16:04:25.876069Z","iopub.status.idle":"2023-12-22T16:04:25.933103Z","shell.execute_reply.started":"2023-12-22T16:04:25.876002Z","shell.execute_reply":"2023-12-22T16:04:25.931661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View shape of the dataset\nprint(f\"The dataset contains {df.shape[0]} transactions and each transaction has a maximum of {df.shape[1]} items or less.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:25.935254Z","iopub.execute_input":"2023-12-22T16:04:25.935747Z","iopub.status.idle":"2023-12-22T16:04:25.942408Z","shell.execute_reply.started":"2023-12-22T16:04:25.935701Z","shell.execute_reply":"2023-12-22T16:04:25.941098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate transaction lists\ntxns = df.fillna(\"\").values.tolist()\ntxns = [[item for item in txn if item != ''] for txn in txns]\ntxns = [[item.strip() for item in txn] for txn in txns]\n\n# Create a list of unique ids for the transactions\nids = [i + 1 for i in range(len(txns))]\n\n# Initialize an empty list\ndata =[]\n# Iterate through transactions and add them to the DataFrame with IDs\nfor i, txn in enumerate(txns):\n    data.extend([{'TID': ids[i], 'Item': item} for item in txn])\n\ndf_txn = pd.DataFrame(data)\ndf_txn.head(25)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:25.943854Z","iopub.execute_input":"2023-12-22T16:04:25.944288Z","iopub.status.idle":"2023-12-22T16:04:26.197681Z","shell.execute_reply.started":"2023-12-22T16:04:25.944253Z","shell.execute_reply":"2023-12-22T16:04:26.196538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the top 30 most frequent items\ntop_items = df_txn['Item'].value_counts().reset_index()\n\n# Convert the top 30 items into DataFrame and sort by item count in descending order\ndf_top_items = pd.DataFrame(top_items)\ndf_top_items.columns = ['Item', 'Count']\n\n# Calculate the percentage of transactions for each item\ntotal_transactions = len(df)\ndf_top_items['% Count'] = (df_top_items['Count']*100 / total_transactions).round(2)\n\n# Display the results\ndf_top_items.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:26.199499Z","iopub.execute_input":"2023-12-22T16:04:26.199921Z","iopub.status.idle":"2023-12-22T16:04:26.257907Z","shell.execute_reply.started":"2023-12-22T16:04:26.199885Z","shell.execute_reply":"2023-12-22T16:04:26.256327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the pyECLAT library\nfrom pyECLAT import ECLAT\n\n# Initiate an Eclat instance and load transactions DataFrame to the instance\neclat = ECLAT(data=df, verbose=True)\n\n# Generate a binary dataframe\neclat.df_bin.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:26.259446Z","iopub.execute_input":"2023-12-22T16:04:26.259822Z","iopub.status.idle":"2023-12-22T16:04:29.178519Z","shell.execute_reply.started":"2023-12-22T16:04:26.259788Z","shell.execute_reply":"2023-12-22T16:04:29.177181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display a list with all the names of the different items\nunique_item_list = eclat.uniq_ \nprint(unique_item_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:29.180063Z","iopub.execute_input":"2023-12-22T16:04:29.180435Z","iopub.status.idle":"2023-12-22T16:04:29.187427Z","shell.execute_reply.started":"2023-12-22T16:04:29.180402Z","shell.execute_reply":"2023-12-22T16:04:29.185897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate frequent itemsets\n# Applying Eclat algorithm assuming an item has to appear in at least 4% of the total transaction to be considered as frequent and a frequent itemset should contain at least 1 item and a maximum of 3 items\nmin_support_threshold = 0.04\nmin_combination = 2\nmax_combination = 3\n\nget_ECLAT_indexes, get_ECLAT_supports = eclat.fit(min_support = min_support_threshold, min_combination = min_combination, max_combination = max_combination, separator=' & ', verbose=True)\n\n# Display results in a dataframe\nresult = pd.DataFrame(get_ECLAT_supports.items(),columns=['Item', 'Support'])\nresult = result.sort_values(by=['Support'], ascending=False).reset_index(drop=True)\nresult","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:04:29.189596Z","iopub.execute_input":"2023-12-22T16:04:29.18995Z","iopub.status.idle":"2023-12-22T16:05:15.993365Z","shell.execute_reply.started":"2023-12-22T16:04:29.189918Z","shell.execute_reply":"2023-12-22T16:05:15.992159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Summary**\n- The top 5 items with the highest support values are: mineral water (23.84%), eggs (17.97%), spaghetti (17.41%), french fries (17.09%), and chocolate (16.38%).<br><p>\n- The least frequent items are fresh bread (4.31%), salmon (4.25%), and ground beef & mineral water (4.09%).<br><p>\n- Some interesting itemsets with relatively high support include mineral water & spaghetti (5.97%), chocolate & mineral water (5.27%), and eggs & mineral water (5.09%).","metadata":{"execution":{"iopub.status.busy":"2023-12-22T15:42:22.043759Z","iopub.execute_input":"2023-12-22T15:42:22.044237Z","iopub.status.idle":"2023-12-22T15:42:22.055503Z","shell.execute_reply.started":"2023-12-22T15:42:22.044197Z","shell.execute_reply":"2023-12-22T15:42:22.053714Z"}}},{"cell_type":"markdown","source":"## **FP-Growth Algorithm**\n---\nThe FP-growth (Frequent Pattern growth) algorithm is another popular algorithm for discovering frequent itemsets and association rules. It offers advantages over both Apriori and Eclat in terms of efficiency and scalability. This page aims to provide an overview of the FP-growth algorithm, its method, key parameters, advantages, limitations, and a basic implementation guide using Python.\n\n\nThe FP-growth algorithm employs a divide-and-conquer strategy to discover frequent itemsets. It constructs a tree-like data structure known as the **FP-tree (Frequent Pattern tree)** from the transaction database. This tree structure compactly represents the frequent patterns and their support counts. FP-growth then **recursively mines the tree to find frequent itemsets**. Unlike Apriori, FP-growth does not generate candidate itemsets explicitly, which makes it faster and more memory-efficient, especially for large datasets. In addition, there is also a **header table**, alongside the FP-tree that stores pointers to the first occurrence of each item in the FP-tree. This speeds up the process of finding frequent itemsets.\n\n### **Key Parameters**\n| **Parameter**             | **Description**                                                               |\n|:--------------------------|:------------------------------------------------------------------------------|\n| `min_support`             | This is the user-defined threshold (a decimal between 0 and 1) that determines the minimum frequency at which an itemset must be present in the dataset to be considered 'frequent'.|\n| `confidence_threshold`    | This is the user-defined minimum level of confidence that an association rule must exceed to be considered significant.\n| `lift_threshold`          | This is the user-defined minimum lift value (typically >1) focusing on rules that have a positive relationship between the antecedent and consequent.|\n","metadata":{}},{"cell_type":"code","source":"# Install machine learning extensions (mlxtend) library\n# !pip install mlxtend","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:15.99492Z","iopub.execute_input":"2023-12-22T16:05:15.995327Z","iopub.status.idle":"2023-12-22T16:05:16.000228Z","shell.execute_reply.started":"2023-12-22T16:05:15.995292Z","shell.execute_reply":"2023-12-22T16:05:15.999114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset and view first 5 rows\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", category=Warning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", None)\n\ndf = pd.read_csv(\"/kaggle/input/market-basket-optimisation/Market_Basket_Optimisation.csv\", header=None, index_col=None, names=[f\"Item_{i}\" for i in range(1, 21)])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.001885Z","iopub.execute_input":"2023-12-22T16:05:16.002266Z","iopub.status.idle":"2023-12-22T16:05:16.057137Z","shell.execute_reply.started":"2023-12-22T16:05:16.002231Z","shell.execute_reply":"2023-12-22T16:05:16.05593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View shape of the dataset\nprint(f\"The dataset contains {df.shape[0]} transactions and each transaction has a maximum of {df.shape[1]} items or less.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.059003Z","iopub.execute_input":"2023-12-22T16:05:16.059903Z","iopub.status.idle":"2023-12-22T16:05:16.066158Z","shell.execute_reply.started":"2023-12-22T16:05:16.059851Z","shell.execute_reply":"2023-12-22T16:05:16.065257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View shape of the dataset\nprint(f\"The dataset contains {df.shape[0]} transactions and each transaction has a maximum of {df.shape[1]} items or less.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.067689Z","iopub.execute_input":"2023-12-22T16:05:16.068232Z","iopub.status.idle":"2023-12-22T16:05:16.083634Z","shell.execute_reply.started":"2023-12-22T16:05:16.068169Z","shell.execute_reply":"2023-12-22T16:05:16.082133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate transaction lists\ntxns = df.fillna(\"\").values.tolist()\ntxns = [[item for item in txn if item != ''] for txn in txns]\ntxns = [[item.strip() for item in txn] for txn in txns]\n\n# Create a list of unique ids for the transactions\nids = [i + 1 for i in range(len(txns))]\n\n# Initialize an empty list\ndata =[]\n# Iterate through transactions and add them to the DataFrame with IDs\nfor i, txn in enumerate(txns):\n    data.extend([{'TID': ids[i], 'Item': item} for item in txn])\n\ndf_txn = pd.DataFrame(data)\ndf_txn.head(25)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.085633Z","iopub.execute_input":"2023-12-22T16:05:16.086089Z","iopub.status.idle":"2023-12-22T16:05:16.328878Z","shell.execute_reply.started":"2023-12-22T16:05:16.086019Z","shell.execute_reply":"2023-12-22T16:05:16.327715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform one hot encoding using TransactionEncoder\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# Create a TransactionEncoder\nte = TransactionEncoder()\n\n# Fit and transform the transaction data\nte_array = te.fit(txns).transform(txns)\n\n# Extract the column names\nte_columns = te.columns_\n\n# Create a DataFrame from the one-hot encoded array\ndf1 = pd.DataFrame(te_array, columns=te.columns_)\n\n# Display the results\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.330329Z","iopub.execute_input":"2023-12-22T16:05:16.330712Z","iopub.status.idle":"2023-12-22T16:05:16.443091Z","shell.execute_reply.started":"2023-12-22T16:05:16.330677Z","shell.execute_reply":"2023-12-22T16:05:16.441964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the top most frequent items\ntop_items = df_txn['Item'].value_counts().reset_index()\n\n# Convert the top items into DataFrame and sort by item count in descending order\ndf_top_items = pd.DataFrame(top_items)\ndf_top_items.columns = ['Item', 'Count']\n\n# Calculate the percentage of transactions for each item\ntotal_transactions = len(df)\ndf_top_items['% Count'] = (df_top_items['Count']*100 / total_transactions).round(2)\n\n# Display the results\ndf_top_items.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.444654Z","iopub.execute_input":"2023-12-22T16:05:16.445749Z","iopub.status.idle":"2023-12-22T16:05:16.503074Z","shell.execute_reply.started":"2023-12-22T16:05:16.445699Z","shell.execute_reply":"2023-12-22T16:05:16.501871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **MLXTEND**","metadata":{}},{"cell_type":"code","source":"# Generate frequent itemsets\nfrom mlxtend.frequent_patterns import fpgrowth\n\n# Applying fpgrowth algorithm assuming an item has to appear in at least 4% of the total transaction to be considered as frequent\nmin_support_threshold = 0.04\nfp_frequent_itemsets = fpgrowth(df1, min_support=min_support_threshold, use_colnames=True)\nfp_frequent_itemsets","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.50435Z","iopub.execute_input":"2023-12-22T16:05:16.504708Z","iopub.status.idle":"2023-12-22T16:05:16.672294Z","shell.execute_reply.started":"2023-12-22T16:05:16.504676Z","shell.execute_reply":"2023-12-22T16:05:16.670933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most frequent individual items are mineral water (23.84%), eggs (17.97%), and spaghetti (17.41%). There are also five frequent itemsets with two items e.g. (chocolate, mineral water) with a support of 5.27%, indicating that these items often appear together in transactions.","metadata":{}},{"cell_type":"code","source":"# Generate association rules from the frequent itemsets assuming the likelihood of purchasing the antecedent, followed by the consequent has to be at least 30% to be considered significant or interesting \nfrom mlxtend.frequent_patterns import association_rules\nconfidence_threshold = 0.3 \nrules = association_rules(fp_frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n\n# Sorting rules by confidence, support, and lift\nsorted_rules = rules.sort_values(['confidence', 'support', 'lift'], ascending=[False, False, False])\nsorted_rules","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.673935Z","iopub.execute_input":"2023-12-22T16:05:16.674427Z","iopub.status.idle":"2023-12-22T16:05:16.705023Z","shell.execute_reply.started":"2023-12-22T16:05:16.67438Z","shell.execute_reply":"2023-12-22T16:05:16.703858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customers who purchase ground beef are 41.66% likely to also purchase mineral water and this association is supported by a lift value of 1.75, which signifies that these items are frequently bought together. Similiar observations were found for some other items - milk, spaghetti, and chocolate, in association with mineral water. To visualize association rules, we can leverage the scatter plots, network graphs, heatmaps, and bar charts to understand their distribution within the dataset and their strength of association.","metadata":{}},{"cell_type":"markdown","source":"### **FPGROWTH_PY**","metadata":{}},{"cell_type":"code","source":"# Install fpgrowth library\n# !pip install fpgrowth_py","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.714082Z","iopub.execute_input":"2023-12-22T16:05:16.714472Z","iopub.status.idle":"2023-12-22T16:05:16.719606Z","shell.execute_reply.started":"2023-12-22T16:05:16.714441Z","shell.execute_reply":"2023-12-22T16:05:16.718303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset and view first 5 rows\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", category=Warning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", None)\n\ndf = pd.read_csv(\"/kaggle/input/market-basket-optimisation/Market_Basket_Optimisation.csv\", header=None, index_col=None, names=[f\"Item_{i}\" for i in range(1, 21)])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.721338Z","iopub.execute_input":"2023-12-22T16:05:16.72199Z","iopub.status.idle":"2023-12-22T16:05:16.772309Z","shell.execute_reply.started":"2023-12-22T16:05:16.721944Z","shell.execute_reply":"2023-12-22T16:05:16.771073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View shape of the dataset\nprint(f\"The dataset contains {df.shape[0]} transactions and each transaction has a maximum of {df.shape[1]} items or less.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.773913Z","iopub.execute_input":"2023-12-22T16:05:16.774286Z","iopub.status.idle":"2023-12-22T16:05:16.780355Z","shell.execute_reply.started":"2023-12-22T16:05:16.774254Z","shell.execute_reply":"2023-12-22T16:05:16.779128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate transaction lists\ntxns = df.fillna(\"\").values.tolist()\ntxns = [[item for item in txn if item != ''] for txn in txns]\ntxns = [[item.strip() for item in txn] for txn in txns]\n\n# Create a list of unique ids for the transactions\nids = [i + 1 for i in range(len(txns))]\n\n# Initialize an empty list\ndata =[]\n# Iterate through transactions and add them to the DataFrame with IDs\nfor i, txn in enumerate(txns):\n    data.extend([{'TID': ids[i], 'Item': item} for item in txn])\n\ndf_txn = pd.DataFrame(data)\ndf_txn.head(25)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.781862Z","iopub.execute_input":"2023-12-22T16:05:16.782286Z","iopub.status.idle":"2023-12-22T16:05:16.903934Z","shell.execute_reply.started":"2023-12-22T16:05:16.78225Z","shell.execute_reply":"2023-12-22T16:05:16.902473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate frequent itemsets\nfrom fpgrowth_py import fpgrowth\n\n# Applying fpgrowth algorithm assuming an item has to appear in at least 4% of the total transaction to be considered as frequent\n# Generate association rules from the frequent itemsets assuming the likelihood of purchasing the antecedent, followed by the consequent has to be at least 30% to be considered significant or interesting \nfrequent_itemsets, rules = fpgrowth(txns, minSupRatio=0.04, minConf=0.3)\nprint(frequent_itemsets)\nprint(f\"\\n{rules}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:16.905619Z","iopub.execute_input":"2023-12-22T16:05:16.9061Z","iopub.status.idle":"2023-12-22T16:05:17.435322Z","shell.execute_reply.started":"2023-12-22T16:05:16.906035Z","shell.execute_reply":"2023-12-22T16:05:17.434126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert each frequent_itemsets set to a string\nfreq_itemsets_str = [', '.join(list(itemset)) for itemset in frequent_itemsets]\n\n# Create DataFrame\nfreq_itemsets_df = pd.DataFrame(freq_itemsets_str, columns=['Itemset'])\nfreq_itemsets_df","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:17.437228Z","iopub.execute_input":"2023-12-22T16:05:17.437993Z","iopub.status.idle":"2023-12-22T16:05:17.453192Z","shell.execute_reply.started":"2023-12-22T16:05:17.437955Z","shell.execute_reply":"2023-12-22T16:05:17.451892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert rules to DataFrame\nrules_df = pd.DataFrame(rules, columns=['Antecedent', 'Consequent', 'Confidence'])\nrules_df = rules_df.sort_values(by=\"Confidence\", ascending =False).reset_index(drop=True)\nrules_df","metadata":{"execution":{"iopub.status.busy":"2023-12-22T16:05:17.454804Z","iopub.execute_input":"2023-12-22T16:05:17.455277Z","iopub.status.idle":"2023-12-22T16:05:17.476807Z","shell.execute_reply.started":"2023-12-22T16:05:17.455238Z","shell.execute_reply":"2023-12-22T16:05:17.475547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Summary**\n- Mineral water is a frequent consequent, suggesting it is commonly bought with many different items.<br><p>\n- The strongest association rule based on confidence is (ground beef) -> (water) with a confidence of about 0.42, meaning that when ground beef is bought, water is also likely to be bought in 42% of the cases.<br><p>\n- Milk, chocolate, and spaghetti also have strong associations with mineral water, with confidence levels above 32%.","metadata":{}},{"cell_type":"markdown","source":"### **Overall Summary of Market Basket Analysis using Apriori, Eclat, and FP-Growth Algorithms**\n---\n- Mineral Water is frequently purchased in combination with a variety of other products, indicating its staple status in consumer transactions.<br>\n- A strong association between ground beef and spaghetti is consistently observed, particularly highlighted in the Apriori and FP-Growth results with a confidence level of approximately 39.89%.<br>\n- The tendency of customers purchasing soup along with mineral water is among the top rules in both Apriori and FP-Growth analyses, showing a confidence level of around 45.66%.<br>\n- All three algorithms show a high degree of consistency in identifying key item associations and trends, particularly concerning the prominence of mineral water and the frequent pairing of The consistency in these results enhances the reliability of the findings and provides a solid foundation for applying these insights in practical retail and marketing scenarios.","metadata":{}}]}